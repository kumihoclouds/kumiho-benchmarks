#!/usr/bin/env python3
"""
Unified benchmark runner for Kumiho Cognitive Memory evaluation.

Runs Tier 1 benchmarks: LoCoMo, LongMemEval, MemoryAgentBench
Produces paper-ready metrics tables and comparison data.

Usage:
    # Run all benchmarks
    python -m kumiho_eval.run_benchmarks --all

    # Run individual benchmarks
    python -m kumiho_eval.run_benchmarks --locomo
    python -m kumiho_eval.run_benchmarks --longmemeval
    python -m kumiho_eval.run_benchmarks --mab

    # Quick smoke test (1 sample each)
    python -m kumiho_eval.run_benchmarks --all --max-samples 1

    # Full run with custom model
    python -m kumiho_eval.run_benchmarks --all --answer-model gpt-4o --judge-model gpt-4o

Environment variables:
    OPENAI_API_KEY          Required for answer generation and LLM judge
    KUMIHO_AUTH_TOKEN       Kumiho server authentication
    KUMIHO_ENDPOINT         Kumiho gRPC endpoint (default: auto-discover)
    KUMIHO_UPSTASH_REDIS_URL  Redis for working memory
    KUMIHO_LLM_API_KEY      LLM for memory summarization
"""

from __future__ import annotations

import argparse
import asyncio
import json
import logging
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

from .common import BenchmarkConfig

logger = logging.getLogger("kumiho_eval")

# Competitor reference scores for paper comparison table
REFERENCE_SCORES = {
    "locomo": {
        "MAGMA": {"judge_accuracy": 0.700, "source": "arXiv 2601.03236"},
        "Mem0": {"judge_accuracy": 0.671, "source": "arXiv 2504.19413"},
        "Mem0+Graph": {"judge_accuracy": 0.657, "source": "arXiv 2504.19413"},
        "Zep/Graphiti": {"judge_accuracy": 0.584, "source": "arXiv 2501.13956 (disputed)"},
        "LangMem": {"judge_accuracy": 0.512, "source": "arXiv 2504.19413"},
        "A-Mem": {"judge_accuracy": 0.406, "source": "arXiv 2504.19413"},
    },
    "longmemeval": {
        "Zep (gpt-4o)": {"accuracy": 0.712, "source": "arXiv 2501.13956"},
        "MAGMA": {"accuracy": 0.612, "source": "arXiv 2601.03236"},
        "Full-context (gpt-4o)": {"accuracy": 0.602, "source": "arXiv 2410.10813"},
        "Zep (gpt-4o-mini)": {"accuracy": 0.638, "source": "arXiv 2501.13956"},
    },
    "memoryagentbench": {
        "MemGPT/Letta": {
            "AR": 31.7, "TTL": 55.3, "LRU": 2.5, "CR_SH": 28.0, "CR_MH": 3.0,
            "source": "arXiv 2507.05257",
        },
        "Mem0": {
            "AR": 29.4, "TTL": 27.0, "LRU": 0.8, "CR_SH": 18.0, "CR_MH": 2.0,
            "source": "arXiv 2507.05257",
        },
        "Cognee": {
            "AR": 27.1, "TTL": 31.9, "LRU": 2.3, "CR_SH": 28.0, "CR_MH": 3.0,
            "source": "arXiv 2507.05257",
        },
        "HippoRAG-v2": {
            "AR": 60.3, "TTL": 41.0, "LRU": 14.6, "CR_SH": 54.0, "CR_MH": 5.0,
            "source": "arXiv 2507.05257",
        },
    },
}


def generate_paper_table(all_metrics: dict[str, Any]) -> str:
    """Generate LaTeX-ready comparison table for the paper."""
    lines = []
    lines.append("% Auto-generated by kumiho_eval benchmark runner")
    lines.append(f"% Date: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M UTC')}")
    lines.append("")

    # LoCoMo table
    if "locomo" in all_metrics:
        m = all_metrics["locomo"]
        lines.append("% --- LoCoMo Results ---")
        lines.append("\\begin{table}[h]")
        lines.append("\\centering")
        lines.append("\\caption{LoCoMo Benchmark Results (LLM-as-Judge Accuracy)}")
        lines.append("\\label{tab:locomo-results}")
        lines.append("\\begin{tabular}{lcc}")
        lines.append("\\toprule")
        lines.append("System & Judge Accuracy & F1 \\\\")
        lines.append("\\midrule")

        # Reference scores
        for name, ref in REFERENCE_SCORES["locomo"].items():
            lines.append(f"{name} & {ref['judge_accuracy']:.3f} & -- \\\\")

        # Our score
        our_judge = m.get("overall_judge_accuracy", 0)
        our_f1 = m.get("overall_f1", 0)
        lines.append("\\midrule")
        lines.append(f"\\textbf{{Kumiho (ours)}} & \\textbf{{{our_judge:.3f}}} & {our_f1:.3f} \\\\")
        lines.append("\\bottomrule")
        lines.append("\\end{tabular}")
        lines.append("\\end{table}")
        lines.append("")

    # LongMemEval table
    if "longmemeval" in all_metrics:
        m = all_metrics["longmemeval"]
        lme = m.get("longmemeval", {})
        lines.append("% --- LongMemEval Results ---")
        lines.append("\\begin{table}[h]")
        lines.append("\\centering")
        lines.append("\\caption{LongMemEval Benchmark Results (GPT-4o Judge Accuracy)}")
        lines.append("\\label{tab:longmemeval-results}")
        lines.append("\\begin{tabular}{lc}")
        lines.append("\\toprule")
        lines.append("System & Overall Accuracy \\\\")
        lines.append("\\midrule")

        for name, ref in REFERENCE_SCORES["longmemeval"].items():
            lines.append(f"{name} & {ref['accuracy']:.3f} \\\\")

        our_acc = lme.get("overall_accuracy", 0)
        lines.append("\\midrule")
        lines.append(f"\\textbf{{Kumiho (ours)}} & \\textbf{{{our_acc:.3f}}} \\\\")
        lines.append("\\bottomrule")
        lines.append("\\end{tabular}")
        lines.append("\\end{table}")
        lines.append("")

    # MemoryAgentBench table
    if "memoryagentbench" in all_metrics:
        m = all_metrics["memoryagentbench"]
        mab = m.get("memoryagentbench", {}).get("per_competency", {})
        lines.append("% --- MemoryAgentBench Results ---")
        lines.append("\\begin{table}[h]")
        lines.append("\\centering")
        lines.append("\\caption{MemoryAgentBench Results by Competency}")
        lines.append("\\label{tab:mab-results}")
        lines.append("\\begin{tabular}{lccccc}")
        lines.append("\\toprule")
        lines.append("System & AR & TTL & LRU & CR-SH & CR-MH \\\\")
        lines.append("\\midrule")

        for name, ref in REFERENCE_SCORES["memoryagentbench"].items():
            lines.append(
                f"{name} & {ref.get('AR', '--')} & {ref.get('TTL', '--')} & "
                f"{ref.get('LRU', '--')} & {ref.get('CR_SH', '--')} & "
                f"{ref.get('CR_MH', '--')} \\\\"
            )

        lines.append("\\midrule")
        ar = mab.get("AR", {}).get("avg_primary_metric", 0) * 100
        ttl = mab.get("TTL", {}).get("avg_primary_metric", 0) * 100
        lru = mab.get("LRU", {}).get("avg_primary_metric", 0) * 100
        cr = mab.get("CR", {}).get("avg_primary_metric", 0) * 100
        lines.append(
            f"\\textbf{{Kumiho (ours)}} & \\textbf{{{ar:.1f}}} & "
            f"\\textbf{{{ttl:.1f}}} & \\textbf{{{lru:.1f}}} & "
            f"\\textbf{{{cr:.1f}}} & -- \\\\"
        )
        lines.append("\\bottomrule")
        lines.append("\\end{tabular}")
        lines.append("\\end{table}")

    return "\n".join(lines)


async def run_all(config: BenchmarkConfig, benchmarks: list[str]) -> dict[str, Any]:
    """Run selected benchmarks and collect all metrics."""
    all_metrics: dict[str, Any] = {}

    if "locomo" in benchmarks:
        from .locomo_eval import evaluate_locomo

        logger.info("=" * 60)
        logger.info("Running LoCoMo benchmark...")
        logger.info("=" * 60)

        locomo_config = BenchmarkConfig(
            **{
                **config.__dict__,
                "project_name": f"{config.project_name}-locomo",
            }
        )
        result = await evaluate_locomo(locomo_config)
        all_metrics["locomo"] = result["metrics"]

    if "longmemeval" in benchmarks:
        from .longmemeval_eval import evaluate_longmemeval

        logger.info("=" * 60)
        logger.info("Running LongMemEval benchmark...")
        logger.info("=" * 60)

        lme_config = BenchmarkConfig(
            **{
                **config.__dict__,
                "project_name": f"{config.project_name}-longmemeval",
            }
        )
        result = await evaluate_longmemeval(lme_config)
        all_metrics["longmemeval"] = result["metrics"]

    if "mab" in benchmarks:
        from .memoryagentbench_eval import evaluate_memoryagentbench

        logger.info("=" * 60)
        logger.info("Running MemoryAgentBench...")
        logger.info("=" * 60)

        mab_config = BenchmarkConfig(
            **{
                **config.__dict__,
                "project_name": f"{config.project_name}-mab",
            }
        )
        result = await evaluate_memoryagentbench(mab_config)
        all_metrics["memoryagentbench"] = result["metrics"]

    # Save combined metrics
    output_dir = Path(config.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    combined_path = output_dir / f"tier1_metrics_{timestamp}.json"
    with open(combined_path, "w") as f:
        json.dump(
            {
                "timestamp": timestamp,
                "config": {
                    "answer_model": config.answer_model,
                    "judge_model": config.judge_model,
                    "recall_limit": config.recall_limit,
                    "recall_mode": config.recall_mode,
                    "max_samples": config.max_samples,
                },
                "metrics": all_metrics,
                "reference_scores": REFERENCE_SCORES,
            },
            f,
            indent=2,
        )
    logger.info("Combined metrics saved to %s", combined_path)

    # Generate paper LaTeX tables
    latex = generate_paper_table(all_metrics)
    latex_path = output_dir / f"paper_tables_{timestamp}.tex"
    with open(latex_path, "w") as f:
        f.write(latex)
    logger.info("LaTeX tables saved to %s", latex_path)

    # Print summary
    print("\n" + "=" * 70)
    print("  TIER 1 BENCHMARK SUMMARY — Kumiho Cognitive Memory")
    print("=" * 70)

    if "locomo" in all_metrics:
        m = all_metrics["locomo"]
        print(f"\n  LoCoMo:")
        print(f"    Judge Accuracy: {m.get('overall_judge_accuracy', 0):.4f}")
        print(f"    Token F1:       {m.get('overall_f1', 0):.4f}")

    if "longmemeval" in all_metrics:
        lme = all_metrics["longmemeval"].get("longmemeval", {})
        print(f"\n  LongMemEval:")
        print(f"    Overall Accuracy:       {lme.get('overall_accuracy', 0):.4f}")
        print(f"    Task-Averaged Accuracy: {lme.get('task_averaged_accuracy', 0):.4f}")

    if "memoryagentbench" in all_metrics:
        mab = all_metrics["memoryagentbench"].get("memoryagentbench", {})
        comp = mab.get("per_competency", {})
        print(f"\n  MemoryAgentBench:")
        for split, vals in comp.items():
            print(f"    {split}: {vals.get('avg_primary_metric', 0) * 100:.1f}%")

    print(f"\n  Output: {output_dir}")
    print("=" * 70 + "\n")

    return all_metrics


def main():
    parser = argparse.ArgumentParser(
        description="Kumiho Cognitive Memory — Benchmark Suite",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run all Tier 1 benchmarks (full recall, default)
  python -m kumiho_eval.run_benchmarks --all

  # Quick smoke test
  python -m kumiho_eval.run_benchmarks --all --max-samples 1

  # Run LoCoMo only
  python -m kumiho_eval.run_benchmarks --locomo

  # Run with summarized recall only
  python -m kumiho_eval.run_benchmarks --all --recall-mode summarized

  # Dual-mode: run full then summarized (overnight)
  python -m kumiho_eval.run_benchmarks --all --dual-mode

  # Run AGM compliance evaluation (Tier 3)
  python -m kumiho_eval.run_benchmarks --agm

  # Run everything (Tier 1 + Tier 3)
  python -m kumiho_eval.run_benchmarks --all --agm

  # Run with specific models
  python -m kumiho_eval.run_benchmarks --all --answer-model gpt-4o-mini --judge-model gpt-4o
""",
    )

    # Benchmark selection
    parser.add_argument("--all", action="store_true", help="Run all Tier 1 benchmarks")
    parser.add_argument("--locomo", action="store_true", help="Run LoCoMo benchmark")
    parser.add_argument("--longmemeval", action="store_true", help="Run LongMemEval benchmark")
    parser.add_argument("--mab", action="store_true", help="Run MemoryAgentBench")
    parser.add_argument("--agm", action="store_true", help="Run AGM compliance evaluation (Tier 3)")

    # Configuration
    parser.add_argument("--output", type=str, default="./results", help="Output directory")
    parser.add_argument("--max-samples", type=int, default=None, help="Limit samples per benchmark")
    parser.add_argument("--answer-model", type=str, default="gpt-4o", help="Model for answer generation")
    parser.add_argument("--judge-model", type=str, default="gpt-4o", help="Model for LLM judge")
    parser.add_argument("--recall-limit", type=int, default=10, help="Max memories to recall per query")
    parser.add_argument("--recall-mode", type=str, default="full", choices=["full", "summarized"],
                        help="Recall mode: full (artifact content) or summarized (title+summary only)")
    parser.add_argument("--dual-mode", action="store_true",
                        help="Run benchmarks twice: once with full recall, once with summarized")
    parser.add_argument("--project", type=str, default="benchmark-eval", help="Kumiho project prefix")
    parser.add_argument("-v", "--verbose", action="store_true", help="Verbose logging")

    args = parser.parse_args()

    # Setup logging
    level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s %(levelname)s %(name)s: %(message)s",
        datefmt="%H:%M:%S",
    )

    # Determine which benchmarks to run
    benchmarks = []
    if args.all:
        benchmarks = ["locomo", "longmemeval", "mab"]
    else:
        if args.locomo:
            benchmarks.append("locomo")
        if args.longmemeval:
            benchmarks.append("longmemeval")
        if args.mab:
            benchmarks.append("mab")

    if not benchmarks and not args.agm:
        parser.print_help()
        print("\nError: specify at least one benchmark (--all, --locomo, --longmemeval, --mab, --agm)")
        sys.exit(1)

    # Determine recall modes to run
    if args.dual_mode:
        recall_modes = ["full", "summarized"]
    else:
        recall_modes = [args.recall_mode]

    t0 = time.time()

    for mode in recall_modes:
        mode_suffix = f"-{mode}" if len(recall_modes) > 1 else ""
        output_dir = f"{args.output}{mode_suffix}" if mode_suffix else args.output

        config = BenchmarkConfig(
            project_name=f"{args.project}{mode_suffix}",
            answer_model=args.answer_model,
            judge_model=args.judge_model,
            output_dir=output_dir,
            max_samples=args.max_samples,
            recall_limit=args.recall_limit,
            recall_mode=mode,
        )

        logger.info("=" * 70)
        logger.info("Starting Tier 1 benchmarks [recall_mode=%s]: %s", mode, ", ".join(benchmarks))
        logger.info("Output directory: %s", output_dir)
        logger.info("=" * 70)

        asyncio.run(run_all(config, benchmarks))

    # --- Tier 3: AGM Compliance ---
    if args.agm:
        from .agm_compliance_eval import evaluate_agm_compliance

        logger.info("=" * 70)
        logger.info("Running AGM Belief Revision Compliance Evaluation (Tier 3)")
        logger.info("=" * 70)

        agm_config = BenchmarkConfig(
            project_name=f"{args.project}-agm",
            output_dir=args.output,
        )

        agm_report = asyncio.run(
            evaluate_agm_compliance(
                config=agm_config,
                output_dir=f"{args.output}/agm",
                max_scenarios=args.max_samples,
            )
        )

        logger.info(
            "AGM Compliance: %d/%d passed (%.0f%%)",
            agm_report.total_passed,
            agm_report.total_scenarios,
            agm_report.overall_pass_rate * 100,
        )

    elapsed = time.time() - t0
    logger.info("Total benchmark time: %.1f seconds (%.1f minutes)", elapsed, elapsed / 60)
    if len(recall_modes) > 1:
        logger.info("Ran both modes: %s", ", ".join(recall_modes))


if __name__ == "__main__":
    main()
